import streamlit as st
import pandas as pd
import json
from typing import Dict, List
import re
import toml
import os
from pathlib import Path
import snowflake.connector

# Configuration de la page
st.set_page_config(
    page_title="SQL Query Optimizer",
    page_icon="üîç",
    layout="wide"
)

st.title("üîç SQL Query Optimizer")
st.markdown("Analysez et optimisez vos requ√™tes SQL les plus co√ªteuses avec l'IA")

# Initialisation de la session Snowflake
@st.cache_data
def load_snowflake_config():
    """Charge les connexions disponibles depuis ~/.snowflake/config.toml"""
    config_path = Path.home() / '.snowflake' / 'config.toml'
    if config_path.exists():
        try:
            config = toml.load(config_path)
            return config
        except Exception as e:
            return None
    return None

@st.cache_resource
def create_snowflake_connection(_conn_params):
    """Cr√©e une connexion Snowflake avec les param√®tres fournis"""
    conn = snowflake.connector.connect(
        account=_conn_params.get('account'),
        user=_conn_params.get('user'),
        password=_conn_params.get('password'),
        database=_conn_params.get('database'),
        schema=_conn_params.get('schema'),
        warehouse=_conn_params.get('warehouse'),
        role=_conn_params.get('role'),
        authenticator=_conn_params.get('authenticator', 'snowflake'),
        client_session_keep_alive=_conn_params.get('client_session_keep_alive', False)
    )

    # Wrapper pour compatibilit√© avec l'API Streamlit
    class SnowflakeConnectionWrapper:
        def __init__(self, conn):
            self._conn = conn

        def cursor(self):
            return self._conn.cursor()

        def close(self):
            return self._conn.close()

    return SnowflakeConnectionWrapper(conn)

def init_session():
    """Initialise la session Snowflake depuis Streamlit in Snowflake ou local"""

    # V√©rifier si d√©j√† connect√© en mode local
    if 'snowflake_connection' in st.session_state:
        return st.session_state['snowflake_connection']

    # Tenter d'abord la connexion Streamlit in Snowflake
    try:
        conn = st.connection("snowflake")
        if 'sis_mode_confirmed' not in st.session_state:
            st.success("‚úÖ Connect√© via Streamlit in Snowflake")
            st.session_state['sis_mode_confirmed'] = True
        return conn
    except Exception as e:
        # Fallback pour d√©veloppement local
        if 'local_mode_shown' not in st.session_state:
            st.info("üìå Mode d√©veloppement local - Connexion depuis config.toml")
            st.session_state['local_mode_shown'] = True

        # Charger les connexions disponibles
        config = load_snowflake_config()
        if not config:
            st.error("‚ùå Fichier ~/.snowflake/config.toml non trouv√© ou invalide")
            return None

        # Extraire les noms de connexion
        connection_names = list(config.keys())
        if not connection_names:
            st.error("‚ùå Aucune connexion trouv√©e dans config.toml")
            return None

        # S√©lection de la connexion via Streamlit
        st.sidebar.header("üîå Configuration de connexion")
        selected_connection = st.sidebar.selectbox(
            "Choisir une connexion:",
            connection_names,
            index=0,
            key="connection_selector"
        )

        if not selected_connection:
            return None

        # R√©cup√©rer les param√®tres de connexion
        conn_params = config[selected_connection]

        # Afficher les d√©tails de connexion (sans le mot de passe)
        st.sidebar.info(f"""
        **Connexion s√©lectionn√©e:** {selected_connection}
        - **Account:** {conn_params.get('account', 'N/A')}
        - **User:** {conn_params.get('user', 'N/A')}
        - **Database:** {conn_params.get('database', 'N/A')}
        - **Schema:** {conn_params.get('schema', 'N/A')}
        - **Warehouse:** {conn_params.get('warehouse', 'N/A')}
        - **Role:** {conn_params.get('role', 'N/A')}
        """)

        # Bouton pour se connecter
        if st.sidebar.button("üîó Se connecter", key="connect_button"):
            try:
                # Cr√©er la connexion Snowflake via fonction cach√©e
                wrapped_conn = create_snowflake_connection(conn_params)
                st.sidebar.success(f"‚úÖ Connect√© √† {selected_connection}")
                st.session_state['snowflake_connection'] = wrapped_conn
                st.rerun()
            except Exception as e:
                st.sidebar.error(f"‚ùå Erreur de connexion: {e}")
                return None

        st.warning("‚ö†Ô∏è Veuillez cliquer sur 'Se connecter' dans la barre lat√©rale")
        return None

def get_expensive_queries():
    """R√©cup√®re les requ√™tes SQL les plus co√ªteuses"""
    query = """
    WITH query_details AS (
        SELECT
            warehouse_name,
            warehouse_size,
            user_name,
            query_id,
            query_text,
            total_elapsed_time,
            start_time,
            end_time,
            -- Identifier la requ√™te la plus longue par combinaison warehouse/user
            ROW_NUMBER() OVER (
                PARTITION BY warehouse_name, user_name
                ORDER BY total_elapsed_time DESC
            ) AS query_rank
        FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY
        WHERE
            warehouse_name IS NOT NULL
            AND execution_status = 'SUCCESS'
            AND START_TIME > DATEADD(DAY, -30, CURRENT_TIMESTAMP())
    ),
    aggregated_queries AS (
        SELECT
            warehouse_name,
            warehouse_size,
            user_name,
            SUM(total_elapsed_time) as total_elapsed_time,
            COUNT(*) as cnt,
            -- Prendre le QUERY_ID et QUERY_TEXT de la requ√™te la plus longue
            MAX(CASE WHEN query_rank = 1 THEN query_id END) as sample_query_id,
            MAX(CASE WHEN query_rank = 1 THEN query_text END) as sample_query_text,
            -- Min et Max des dates d'ex√©cution
            MIN(start_time) as min_start_time,
            MAX(end_time) as max_end_time,
            ROW_NUMBER() OVER (
                PARTITION BY warehouse_name
                ORDER BY SUM(total_elapsed_time) DESC
            ) AS rank
        FROM query_details
        GROUP BY warehouse_name, warehouse_size, user_name
    )
    SELECT
        warehouse_name,
        warehouse_size,
        user_name,
        cnt,
        sample_query_id,
        sample_query_text,
        min_start_time,
        max_end_time,
        total_elapsed_time / 1000 AS duration_seconds,
        total_elapsed_time / 1000 / 60 / 60 AS duration_hours,
        total_elapsed_time / 1000 / 60 / 60 *
            CASE
                WHEN warehouse_size = 'X-Small' THEN 1
                WHEN warehouse_size = 'Small' THEN 2
                WHEN warehouse_size = 'Medium' THEN 4
                WHEN warehouse_size = 'Large' THEN 8
                WHEN warehouse_size = 'X-Large' THEN 16
                WHEN warehouse_size = '2X-Large' THEN 32
                ELSE 1
            END AS cost_factor
    FROM aggregated_queries
    WHERE rank <= 20
    ORDER BY duration_seconds DESC
    """
    return query

def get_query_details(query_id: str, conn):
    """R√©cup√®re les d√©tails d'une requ√™te sp√©cifique"""
    query = """
    SELECT
        QUERY_ID,
        QUERY_TEXT,
        QUERY_TYPE,
        WAREHOUSE_NAME,
        WAREHOUSE_SIZE,
        USER_NAME,
        ROLE_NAME,
        DATABASE_NAME,
        SCHEMA_NAME,
        TOTAL_ELAPSED_TIME / 1000 AS duration_seconds,
        BYTES_SCANNED,
        BYTES_SPILLED_TO_LOCAL_STORAGE,
        BYTES_SPILLED_TO_REMOTE_STORAGE,
        PARTITIONS_SCANNED,
        PARTITIONS_TOTAL,
        ROWS_PRODUCED,
        ROWS_INSERTED,
        ROWS_UPDATED,
        ROWS_DELETED,
        COMPILATION_TIME / 1000 AS compilation_time_seconds,
        EXECUTION_TIME / 1000 AS execution_time_seconds,
        QUEUED_OVERLOAD_TIME / 1000 AS queued_time_seconds,
        TRANSACTION_BLOCKED_TIME / 1000 AS blocked_time_seconds,
        START_TIME,
        END_TIME,
        EXECUTION_STATUS
    FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY
    WHERE QUERY_ID = ?
    """
    
    try:
        cursor = conn.cursor()
        cursor.execute(query, (query_id,))
        df = cursor.fetch_pandas_all()
        # Normaliser les noms de colonnes en minuscules
        df.columns = df.columns.str.lower()
        return df
    except Exception as e:
        st.error(f"Erreur lors de la r√©cup√©ration des d√©tails de la requ√™te: {str(e)}")
        return None

def get_query_text_by_user_warehouse(user_name: str, warehouse_name: str, conn):
    """R√©cup√®re le texte SQL d'une requ√™te bas√©e sur user et warehouse"""
    query = """
    SELECT
        QUERY_ID,
        QUERY_TEXT,
        QUERY_TYPE,
        WAREHOUSE_NAME,
        WAREHOUSE_SIZE,
        USER_NAME,
        ROLE_NAME,
        DATABASE_NAME,
        SCHEMA_NAME,
        TOTAL_ELAPSED_TIME / 1000 AS duration_seconds,
        BYTES_SCANNED,
        BYTES_SPILLED_TO_LOCAL_STORAGE,
        BYTES_SPILLED_TO_REMOTE_STORAGE,
        PARTITIONS_SCANNED,
        PARTITIONS_TOTAL,
        ROWS_PRODUCED,
        ROWS_INSERTED,
        ROWS_UPDATED,
        ROWS_DELETED,
        COMPILATION_TIME / 1000 AS compilation_time_seconds,
        EXECUTION_TIME / 1000 AS execution_time_seconds,
        QUEUED_OVERLOAD_TIME / 1000 AS queued_time_seconds,
        TRANSACTION_BLOCKED_TIME / 1000 AS blocked_time_seconds,
        START_TIME,
        END_TIME,
        EXECUTION_STATUS
    FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY
    WHERE USER_NAME = ? 
        AND WAREHOUSE_NAME = ?
        AND EXECUTION_STATUS = 'SUCCESS'
        AND START_TIME > DATEADD(DAY, -30, CURRENT_TIMESTAMP())
    ORDER BY TOTAL_ELAPSED_TIME DESC
    LIMIT 1
    """
    
    try:
        cursor = conn.cursor()
        cursor.execute(query, (user_name, warehouse_name))
        df = cursor.fetch_pandas_all()
        # Normaliser les noms de colonnes en minuscules
        df.columns = df.columns.str.lower()
        return df
    except Exception as e:
        st.error(f"Erreur lors de la r√©cup√©ration du texte de la requ√™te: {str(e)}")
        return None

def extract_tables_from_sql(sql_text: str) -> List[str]:
    """Extrait les noms de tables depuis le texte SQL"""
    # Pattern pour identifier les tables (FROM, JOIN, etc.)
    # Support pour database.schema.table, schema.table, ou table
    patterns = [
        r'FROM\s+([\w\.`"]+)',
        r'JOIN\s+([\w\.`"]+)',
        r'INTO\s+([\w\.`"]+)',
        r'UPDATE\s+([\w\.`"]+)',
        r'MERGE\s+INTO\s+([\w\.`"]+)',
        r'TABLE\s+([\w\.`"]+)',
    ]
    
    tables = set()
    for pattern in patterns:
        matches = re.findall(pattern, sql_text, re.IGNORECASE)
        for match in matches:
            # Nettoyer le nom de la table (enlever backticks, guillemets, espaces)
            table = match.strip().strip('`').strip('"').strip()
            # Filtrer les alias (AS alias) et les mots-cl√©s SQL
            if table and table.upper() not in ['AS', 'ON', 'WHERE', 'GROUP', 'ORDER', 'HAVING', 'SELECT']:
                # Enlever les alias potentiels apr√®s un espace
                table = table.split()[0] if ' ' in table else table
                if table and not table.startswith('('):
                    tables.add(table)
    
    return sorted(list(tables))


@st.cache_data
def fetch_pandas_all(expensive_queries_sql):
    return conn.cursor().execute(expensive_queries_sql).fetch_pandas_all()

def get_table_metadata(table_name: str, conn):
    """R√©cup√®re les m√©tadonn√©es d'une table"""
    # S√©parer database.schema.table si n√©cessaire
    parts = table_name.split('.')
    
    if len(parts) == 3:
        database, schema, table = parts
    elif len(parts) == 2:
        database = None
        schema, table = parts
    else:
        database = None
        schema = None
        table = table_name
    
    metadata = {}
    
    try:
        # R√©cup√©rer les colonnes de la table
        if database and schema:
            query_columns = f"""
            SELECT 
                COLUMN_NAME,
                DATA_TYPE,
                IS_NULLABLE,
                COLUMN_DEFAULT,
                COMMENT
            FROM {database}.INFORMATION_SCHEMA.COLUMNS
            WHERE TABLE_SCHEMA = '{schema}' AND TABLE_NAME = '{table}'
            ORDER BY ORDINAL_POSITION
            """
        elif schema:
            query_columns = f"""
            SELECT 
                COLUMN_NAME,
                DATA_TYPE,
                IS_NULLABLE,
                COLUMN_DEFAULT,
                COMMENT
            FROM INFORMATION_SCHEMA.COLUMNS
            WHERE TABLE_SCHEMA = '{schema}' AND TABLE_NAME = '{table}'
            ORDER BY ORDINAL_POSITION
            """
        else:
            query_columns = f"""
            SELECT 
                COLUMN_NAME,
                DATA_TYPE,
                IS_NULLABLE,
                COLUMN_DEFAULT,
                COMMENT
            FROM INFORMATION_SCHEMA.COLUMNS
            WHERE TABLE_NAME = '{table}'
            ORDER BY ORDINAL_POSITION
            """
        
        cursor = conn.cursor()
        cursor.execute(query_columns)
        columns_df = cursor.fetch_pandas_all()
        # Normaliser les noms de colonnes en minuscules
        if not columns_df.empty:
            columns_df.columns = columns_df.columns.str.lower()
        metadata['columns'] = columns_df.to_dict('records') if not columns_df.empty else []
        
        # R√©cup√©rer les statistiques de la table
        if database and schema:
            query_stats = f"""
            SELECT 
                ROW_COUNT,
                BYTES,
                RETENTION_TIME,
                CREATED,
                LAST_ALTERED
            FROM {database}.INFORMATION_SCHEMA.TABLES
            WHERE TABLE_SCHEMA = '{schema}' AND TABLE_NAME = '{table}'
            """
        elif schema:
            query_stats = f"""
            SELECT 
                ROW_COUNT,
                BYTES,
                RETENTION_TIME,
                CREATED,
                LAST_ALTERED
            FROM INFORMATION_SCHEMA.TABLES
            WHERE TABLE_SCHEMA = '{schema}' AND TABLE_NAME = '{table}'
            """
        else:
            query_stats = f"""
            SELECT 
                ROW_COUNT,
                BYTES,
                RETENTION_TIME,
                CREATED,
                LAST_ALTERED
            FROM INFORMATION_SCHEMA.TABLES
            WHERE TABLE_NAME = '{table}'
            """
        
        cursor.execute(query_stats)
        stats_df = cursor.fetch_pandas_all()
        # Normaliser les noms de colonnes en minuscules
        if not stats_df.empty:
            stats_df.columns = stats_df.columns.str.lower()
        metadata['statistics'] = stats_df.to_dict('records')[0] if not stats_df.empty else {}
        
        # R√©cup√©rer les cl√©s primaires et index
        if database and schema:
            query_keys = f"""
            SELECT 
                CONSTRAINT_NAME,
                CONSTRAINT_TYPE
            FROM {database}.INFORMATION_SCHEMA.TABLE_CONSTRAINTS
            WHERE TABLE_SCHEMA = '{schema}' AND TABLE_NAME = '{table}'
            """
        elif schema:
            query_keys = f"""
            SELECT 
                CONSTRAINT_NAME,
                CONSTRAINT_TYPE
            FROM INFORMATION_SCHEMA.TABLE_CONSTRAINTS
            WHERE TABLE_SCHEMA = '{schema}' AND TABLE_NAME = '{table}'
            """
        else:
            query_keys = f"""
            SELECT 
                CONSTRAINT_NAME,
                CONSTRAINT_TYPE
            FROM INFORMATION_SCHEMA.TABLE_CONSTRAINTS
            WHERE TABLE_NAME = '{table}'
            """
        
        cursor.execute(query_keys)
        keys_df = cursor.fetch_pandas_all()
        # Normaliser les noms de colonnes en minuscules
        if not keys_df.empty:
            keys_df.columns = keys_df.columns.str.lower()
        metadata['constraints'] = keys_df.to_dict('records') if not keys_df.empty else []
        
    except Exception as e:
        st.warning(f"Erreur lors de la r√©cup√©ration des m√©tadonn√©es pour {table_name}: {str(e)}")
        metadata['error'] = str(e)
    
    return metadata

def call_cortex_ai(query_text: str, execution_metadata: Dict, tables_metadata: Dict[str, Dict], conn):
    """Appelle Cortex AI (Claude Sonnet) pour obtenir des suggestions d'optimisation"""
    
    # Pr√©parer le prompt
    prompt = f"""Tu es un expert en optimisation de requ√™tes SQL sur Snowflake. 

Analyse la requ√™te SQL suivante et fournis des suggestions d'optimisation d√©taill√©es.

## Requ√™te SQL √† analyser :

```sql
{query_text}
```

## M√©tadonn√©es d'ex√©cution :

{json.dumps(execution_metadata, indent=2, default=str)}

## M√©tadonn√©es des tables utilis√©es :

{json.dumps(tables_metadata, indent=2, default=str)}

## Instructions :

Fournis une analyse compl√®te avec :

1. **Optimisations SQL** :
   - Suggestions de r√©√©criture de la requ√™te
   - Am√©lioration des JOINs
   - Optimisation des WHERE clauses
   - Utilisation d'index ou de clustering keys
   - Suggestions de CTEs ou de sous-requ√™tes

2. **Optimisations li√©es au Warehouse** :
   - Taille de warehouse recommand√©e
   - Utilisation de multi-clustering
   - Auto-suspend et auto-resume
   - Gestion de la concurrence

3. **Optimisations g√©n√©rales** :
   - Am√©lioration du temps d'ex√©cution
   - R√©duction des co√ªts
   - Meilleures pratiques Snowflake

Formatte ta r√©ponse de mani√®re claire et structur√©e avec des sections bien d√©finies."""

    try:
        # √âchapper les apostrophes pour SQL
        escaped_prompt = prompt.replace("'", "''")

        # Appel √† Cortex AI via SNOWFLAKE.CORTEX.COMPLETE
        # Nouvelle syntaxe: SNOWFLAKE.CORTEX.COMPLETE(model_name, prompt_text, options)
        cortex_query = f"""
        SELECT SNOWFLAKE.CORTEX.COMPLETE(
            'claude-4-sonnet',
            '{escaped_prompt}'
        ) AS response
        """

        cursor = conn.cursor()
        cursor.execute(cortex_query)
        result = cursor.fetchone()
        
        if result and result[0]:
            # Le r√©sultat de CORTEX.COMPLETE est directement le texte g√©n√©r√©
            response = result[0]

            # Si c'est d√©j√† une string, la retourner directement
            if isinstance(response, str):
                return response

            # Si c'est un dict, essayer d'extraire le contenu
            if isinstance(response, dict):
                # V√©rifier diff√©rents formats possibles
                if 'choices' in response and len(response['choices']) > 0:
                    choice = response['choices'][0]
                    if isinstance(choice, dict):
                        if 'message' in choice and 'content' in choice['message']:
                            return choice['message']['content']
                        if 'text' in choice:
                            return choice['text']
                if 'content' in response:
                    return response['content']
                if 'text' in response:
                    return response['text']

                # Si rien n'a fonctionn√©, retourner le JSON format√©
                return json.dumps(response, indent=2, ensure_ascii=False)

            # Dernier recours: convertir en string
            return str(response)
        else:
            return None
            
    except Exception as e:
        st.error(f"Erreur lors de l'appel √† Cortex AI: {str(e)}")
        st.exception(e)
        return None

# Interface principale
conn = init_session()

if conn is None:
    st.stop()

# Section 1: Liste des requ√™tes co√ªteuses
st.header("üìä Requ√™tes SQL les plus co√ªteuses")

if st.button("üîÑ Actualiser la liste"):
    st.cache_data.clear()

try:
    expensive_queries_sql = get_expensive_queries()
    df_queries = fetch_pandas_all(expensive_queries_sql)
    # Normaliser les noms de colonnes en minuscules (Snowflake retourne en majuscules)
    df_queries.columns = df_queries.columns.str.lower()

    if df_queries.empty:
        st.info("Aucune requ√™te co√ªteuse trouv√©e.")
    else:
        # Convertir les colonnes num√©riques (Snowflake peut retourner des strings)
        numeric_cols = ['duration_seconds', 'duration_hours', 'cost_factor', 'cnt']
        for col in numeric_cols:
            if col in df_queries.columns:
                df_queries[col] = pd.to_numeric(df_queries[col], errors='coerce')

        # Formatage des donn√©es pour l'affichage
        display_df = df_queries.copy()
        display_df['duration_seconds'] = display_df['duration_seconds'].round(2)
        display_df['duration_hours'] = display_df['duration_hours'].round(4)
        display_df['cost_factor'] = display_df['cost_factor'].round(2)

        # Formater les dates pour l'affichage
        if 'min_start_time' in display_df.columns:
            display_df['min_start_time'] = pd.to_datetime(display_df['min_start_time'])
        if 'max_end_time' in display_df.columns:
            display_df['max_end_time'] = pd.to_datetime(display_df['max_end_time'])

        # Cr√©er une version simplifi√©e pour l'affichage avec seulement les colonnes demand√©es
        table_display_df = display_df[['warehouse_name', 'warehouse_size', 'user_name', 'cnt', 'duration_seconds']].copy()

        # Layout en deux colonnes
        col_left, col_right = st.columns([1, 1])

        with col_left:
            event = st.dataframe(
                table_display_df,
                use_container_width=True,
                hide_index=True,
                on_select="rerun",
                selection_mode="single-row" #play all rows without pagination
            )

        with col_right:
            st.subheader("üíª D√©tails SQL")
            # Afficher les d√©tails SQL si une ligne est s√©lectionn√©e
            if event.selection and len(event.selection.rows) > 0:
                selected_idx = event.selection.rows[0]
                selected_row = display_df.iloc[selected_idx]

                # M√©triques compl√©mentaires
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Facteur de co√ªt", f"{selected_row['cost_factor']:.2f}")
                with col2:
                    if pd.notna(selected_row.get('min_start_time')):
                        st.metric("Premi√®re exec.", selected_row['min_start_time'].strftime('%Y-%m-%d %H:%M'))
                with col3:
                    if pd.notna(selected_row.get('max_end_time')):
                        st.metric("Derni√®re exec.", selected_row['max_end_time'].strftime('%Y-%m-%d %H:%M'))

                # Afficher le texte SQL
                if 'sample_query_text' in selected_row and pd.notna(selected_row['sample_query_text']):
                    st.code(selected_row['sample_query_text'], language='sql', line_numbers=True)
                else:
                    st.info("Aucun texte SQL disponible pour cette requ√™te")

                # Bouton pour analyser cette requ√™te avec l'IA
                if st.button("üöÄ Analyser cette requ√™te avec l'IA", use_container_width=True):
                    # Utiliser directement les donn√©es de selected_row
                    query_text = selected_row['sample_query_text']
                    query_id = selected_row.get('sample_query_id', 'N/A')

                    # Extraire les tables
                    with st.spinner("Identification des tables utilis√©es..."):
                        tables = extract_tables_from_sql(query_text)

                    if tables:
                        # R√©cup√©rer les m√©tadonn√©es des tables
                        with st.spinner("R√©cup√©ration des m√©tadonn√©es des tables..."):
                            tables_metadata = {}
                            for table in tables:
                                tables_metadata[table] = get_table_metadata(table, conn)

                            # Pr√©parer les m√©tadonn√©es d'ex√©cution √† partir de selected_row
                            execution_metadata = {
                                'query_id': query_id,
                                'duration_seconds': float(selected_row['duration_seconds']),
                                'warehouse_name': selected_row['warehouse_name'],
                                'warehouse_size': selected_row['warehouse_size'],
                                'user_count': int(selected_row['cnt']),
                                'cost_factor': float(selected_row['cost_factor']),
                                'min_start_time': str(selected_row['min_start_time']) if pd.notna(selected_row.get('min_start_time')) else None,
                                'max_end_time': str(selected_row['max_end_time']) if pd.notna(selected_row.get('max_end_time')) else None,
                                # Note: detailed metrics like bytes_scanned not available in selected_row
                            }

                            # Appel √† Cortex AI
                            with st.spinner("Analyse par Cortex AI (Claude Sonnet)..."):
                                optimization_suggestions = call_cortex_ai(
                                    query_text,
                                    execution_metadata,
                                    tables_metadata,
                                    conn
                                )

                                # Stocker les r√©sultats dans session state pour affichage en-dessous
                                st.session_state['ai_analysis'] = {
                                    'tables': tables,
                                    'suggestions': optimization_suggestions
                                }
                    else:
                        st.session_state['ai_analysis'] = {
                            'tables': [],
                            'suggestions': None
                        }
            else:
                st.info("üëà S√©lectionnez une ligne dans le tableau pour voir le code SQL")

        # Afficher les r√©sultats de l'analyse IA en-dessous des deux colonnes
        if 'ai_analysis' in st.session_state and st.session_state['ai_analysis'] is not None:
            st.divider()
            st.header("ü§ñ Analyse IA")

            analysis = st.session_state['ai_analysis']

            if analysis['tables']:
                st.subheader("üìã Tables identifi√©es")
                st.write(", ".join(analysis['tables']))

            if analysis['suggestions']:
                st.subheader("‚ú® Suggestions d'optimisation")
                st.markdown(analysis['suggestions'])
            elif analysis['tables'] is not None and len(analysis['tables']) == 0:
                st.warning("Aucune table identifi√©e dans la requ√™te SQL.")
            elif analysis['suggestions'] is None and analysis['tables']:
                st.warning("Impossible d'obtenir des suggestions d'optimisation. V√©rifiez que Cortex AI est activ√© dans votre compte Snowflake.")

except Exception as e:
    st.error(f"Erreur lors de l'ex√©cution de la requ√™te: {str(e)}")
    st.exception(e)

